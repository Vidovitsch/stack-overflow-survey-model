{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Job Satisfaction of Software Developers\n",
    "In this research project a classification model will be created that tries to predict the Job Satisfaction of Software Developers using the survey results from the Stack Overflow survey of 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Describing the dataset\n",
    "[Stack Overflow](https://stackoverflow.com/) is a website where you can ask and answer software related questions. It is a platform where millions of programmers, software developers, software engineers, etc. meet every day to learn form each other. Stack Overflow itself is aware of the enormous popularity of their platform and for this reason they keep a annual survey to get general insight about the average software engineer in relation to his/her field of work.\n",
    "\n",
    "Every year the results of the survey will get published on their website (cleaned and in csv format). Multiple datasets (one for each year since 2015) are available for analysis. For this project the survey results of 2018 will be used exclusively. The main reason for this choice is recency and the completeness of the results. The 2018 survey results were filled in by around 100.000 software developers, most of which answered 129 different questions. These questions are about job satisfaction, salary, favourite programming languages, weekly exercise, company size, etc. This large variety of questions provides a source of interesting research opportunities.\n",
    "\n",
    "**Source of the dataset**<br />\n",
    "Stack Overflow Developer Survey 2018 (186 MB): https://www.kaggle.com/stackoverflow/stack-overflow-2018-developer-survey\n",
    "\n",
    "\n",
    "**Data description:**<br />\n",
    "The Stack Overflow survey results of 2018 has in total 98855 rows and 129 columns. Some of these columns consist of only numerical data like 'Salary' and 'ConvertedSalary', while all the other columns are categorical (nominal or ordinal). The categorical columns are devidable over three data types:\n",
    "- Values denoted in text. E.g. 'Yes', 'No', 'United States', 'Employed part-time', etc.\n",
    "- Values denoted in ';' seperated lists. E.g. 'Python;Java;C#', 'Windows;Linux;, etc.\n",
    "- Values denoted in numbers. E.g. 1, 2, 3, 4, etc. (for rankings)\n",
    "\n",
    "Textual input can't be interpreted easily by the average Machine Learning algorithm. Therefore, preprocessing of the original dataset is needed, so it can be used for further analysis. A [notebook](./dataset_preprocessing.ipynb) is created that is dedicted to preprocessing the Stack Overflow survey dataset in the following ways:\n",
    "- Drop rows with missing values in the column 'JobSatisfaction'. The column 'JobSatisfaction' is the target value that will be used for analysis. It is not desirable to have missing values for a target feature, because the value NaN doesn't refer to valid classification value.\n",
    "- Drop unimportant columns. Some columns can be left out because they have no correlation with the target column 'JobSatisfaction', are redundant or have too many missing values (35% or higher).\n",
    "- Preprocess values dentoed in ';' seperated lists. List values such as 'Python;Java;C#' can't be used as input for a Machine Learning algorithm. First, the value has to be numerical. Second, numerification of the ';' value as is will result a unique class for every unique list. It is instead needed to get a unique class for every language present in the list.\n",
    "- Encode text to numerical values. Text isn't easy to interpret for Machine Learning algorithms. To solve this problem all text-formatted values will be converted to numerical values. Nominal values are encoded with One Hot Encoding, while ordinal values are encoded with manually added labels.\n",
    "- Impute missing values. A lot of data is missing, this missing data can be imputed with statistical values (e.g. mean, mode, etc.)\n",
    "\n",
    "The above steps will result in a preprocessed data set with 69276 rows and 623 columns.<br />\n",
    "The data will be tranformed as follows:<br />\n",
    "\n",
    "| Student | Programming Language | Country        |\n",
    "|:-------:|:--------------------:|:--------------:|\n",
    "| Yes     | Python;Java;C#       | Kenya          |\n",
    "| No      | Python;C#            | United Kingdom |\n",
    "| Yes     | Java;C#              | United States  |\n",
    "\n",
    "| Student | Python | Java | C# | Country%Kenya | Country%United Kingdom | Country%United States |\n",
    "|:-------:|:------:|:----:|:--:|:-------------:|:----------------------:|:---------------------:|\n",
    "| 1       | 1      | 1    | 1  | 1             | 0                      | 0                     |\n",
    "| 0       | 1      | 0    | 1  | 0             | 1                      | 0                     |\n",
    "| 1       | 0      | 1    | 1  | 0             | 0                      | 1                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining research\n",
    "Research will be conducted on the job satisfaction of software developers. The dataset, as described above, will be used to create a classifaction model that predicts the job statisfaction of software developers. The following research question will be answered:\n",
    "\n",
    "**Can an accurate model be created, given the features denoted in the survey, to predict the job satisfaction of software developers?**\n",
    "\n",
    "The model will be seen as accurate if at least 90% of the predictions are the same as the target values (column 'JobSatisfaction'). On top of the research question, a initial hypothesis can be made:\n",
    "\n",
    "**Salary and career satisfaction are the main influencors of job satisfaction, and is therefore responsible for a high accuracy classification model.**\n",
    "\n",
    "This hypothesis will be either approved or rejected accoring to the reseach results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset exploration\n",
    "Initial exploration of the data gives insight of the data itself and a better intuiting while conducting the research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# disable chained assignments\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Import the preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e5f6efa119b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import preprocessed dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mso_survey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./dataset/so_survey_prepped.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Import the mappings for decoding purposes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mso_mappings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./dataset/so_survey_mappings.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m     \"\"\"\n\u001b[0;32m    813\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import preprocessed dataset\n",
    "so_survey = pd.read_csv('./dataset/so_survey_prepped.csv')\n",
    "\n",
    "# Import the mappings for decoding purposes\n",
    "so_mappings = pd.read_csv('./dataset/so_survey_mappings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first three entries of so_survey data frame\n",
    "so_survey.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first three entries of so_mappings data frame\n",
    "so_mappings.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Create decode  and encode function\n",
    "In advance, a generic decode and encode function will be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_labels(labels, column_name, decoder=so_mappings):\n",
    "    \"\"\" Decodes encoded (preprocessed) labes using a decoder.\n",
    "    E.g. [0, 1, 0, 0] for column 'Hobby' => ['Yes', 'No', 'Yes', 'Yes']\n",
    "    \"\"\"\n",
    "    decoded_labels = so_mappings[column_name].values\n",
    "    return [decoded_labels[i] for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels, column_name, encoder=so_mappings):\n",
    "    \"\"\" Encoded decoded labes using an encoder.\n",
    "    E.g. ['Yes', 'No', 'Yes', 'Yes'] for column 'Hobby' => [0, 1, 0, 0]\n",
    "    \"\"\"\n",
    "    decoded_labels = so_mappings[column_name].values.tolist()\n",
    "    return [decoded_labels.index(l) for l in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Distribution of salary\n",
    "Lets see how salary is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a data frame with column 'ConvertedSalary' and NaN values filtered out\n",
    "mask = pd.isnull(so_survey['ConvertedSalary']) == False\n",
    "salaries = so_survey['ConvertedSalary'][mask].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot salary distribution\n",
    "plt.figure(figsize=(20,5))\n",
    "kde = sns.kdeplot(salaries, shade=True)\n",
    "kde.set(title='Data distribution of salary', \n",
    "        xlabel='Salary', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of salary appears the be very skewed. To reduce the skew, log transformation can be applied on the salaries. But before we can do this, the salaries with value zero needs to be filtered out (logarithm of zero is impossible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries = np.log(salaries[salaries != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot salary distribution (log transformed)\n",
    "plt.figure(figsize=(20,5))\n",
    "kde = sns.kdeplot(salaries, shade=True)\n",
    "kde.set(title='Data distribution of salary (log transformed)', \n",
    "        xlabel='Salary (log transformed)', \n",
    "        ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data distribution has now more the appearance of a gaussian distribution (bell curve shape). Although, the data is still skewed due to the outliers. If the many outliers will cause the classification model to perform badly, the outliers will be removed. This can be done, for example, by pruning the data by $x$ times the standard deviation of the salaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Research strategy\n",
    "The research will be conducted by using following steps:\n",
    "1. Loosely optimize the hyperparameters of a classifier that will be used in the feature selection part. This prevents selecting features with e.g. an overfitted/underfit model.\n",
    "2. Selecting features with the finetuned classifier. (feature selection is necessary since training classifiers with 409 features and more than 10000 rows is extremely time consuming) \n",
    "3. Use the selected features to train and optimize different Machine Learning algorithms.\n",
    "4. Compare the performance of the different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary sklearn libraries\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target (y) \n",
    "target = 'JobSatisfaction'\n",
    "y = so_survey[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set features (X)\n",
    "features = [x for x in so_survey.columns if x != target]\n",
    "X = so_survey[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Perform feature scaling\n",
    "The column 'ConvertedSalary' will be log tranformed to reduce skewness. Also, a min max scaler will be used to scale the features to values between 0 and 1, resulting in all features having values within the same range. This has some benefits:\n",
    "+ All features have the same 'weight'. This will help algorithms like SVM (with Gaussian kernel) and K-means to perform better.\n",
    "+ Speeds up Gradient Descent while training.\n",
    "+ Improves insight in coefficients. If all features have the same 'weight', the resulting coefficients will describe the influence of feature on the target better. This is useful for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the logarithm of 0 is impossible.\n",
    "# For this reason the values of 0 will be replaced by 0.1.\n",
    "X['ConvertedSalary'].loc[(X['ConvertedSalary'] == 0)] = 0.1\n",
    "\n",
    "# Perform log transformation on the values in the column 'ConvertedSalary'\n",
    "X['ConvertedSalary'] = np.log(X['ConvertedSalary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature scaling with min max scaler\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Get training and testing sets\n",
    "The features and target values will be split into training and testing sets.\n",
    "The training set will contain 75% of the dataset (randomly selected rows).\n",
    "The testing set will therefore contain 25% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loosely optimize estimator\n",
    "The Random Forest classifier will be used as an estimator for the feature selection step. This has the following benefits:\n",
    "+ Popular feature selection algorithms like RFE or RFECV are recursively training a classifier. This will take a lot of time. Decision Trees are faster than classifiers such as Neural Networks or Support Vector Machines. \n",
    "+ By using a Random Forest the main drawback of Decision Trees (overfitting the training set) will be (partly) solved.\n",
    "+ It isn't known if the classes are linear separatable. A Random Forest is both a linear and a non-linear classifier.\n",
    "<br><br>The RandomizedSearch method will be used in contrast to GridSearch, because RandomizedSearch doesn't compute all possible computations (hence 'loosely') and is therefore computationally less expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Random Forest classifier and RandomizedSearchCV algorithm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random grid with hyperparameter ranges\n",
    "random_grid = { 'n_estimators': np.arange(50, 250, 50),\n",
    "                'max_depth': [5],\n",
    "                'min_samples_split': np.arange(50, 105, 5),\n",
    "                'min_samples_leaf': np.arange(20, 55, 5) }\n",
    "# Print grid\n",
    "[print('%s: %s' % x) for x in random_grid.items()];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train default Random Forest classifier for comparison\n",
    "default_score = np.sum(cross_val_score(RandomForestClassifier(random_state=21), X, y, cv=5)) / 5\n",
    "\n",
    "# Print default cross-validated accuracy\n",
    "print('Mean cross-validated accuracy of the default estimator: %0.3f' % (default_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random Search of optimal set of hyperparameters using 3 fold cross validation and 25 different combinations. \n",
    "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=21), \n",
    "                                   param_distributions=random_grid, \n",
    "                                   n_iter=25, \n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=21, \n",
    "                                   n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Random Search on the training set\n",
    "random_search = random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best parameters\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print best cross-validated accuracy\n",
    "print('Mean cross-validated accuracy of the best estimator: %0.3f' % (random_search.best_score_))\n",
    "\n",
    "# Print improvements relative to Random Forest classfier with default hyperparameters\n",
    "improvement = random_search.best_score_ - default_score\n",
    "print('Improvement relative to default mean cross-validated accuracy: : %0.3f' % (improvement))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature selection\n",
    "The datasets contains 622 features and about 69000 rows. Training with such an amount of features can take a very long time for algorithms like SVM (expontential time complexity). The training process becomes much faster by pruning redundant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. RFECV (Recursive Feature Elimination with Cross-Validation)\n",
    "Recursive Feature Elimination is a method that uses an estimator (in this case a classifier) that assigns weights to features (coefficients). The goal is to recursively select features by considering smaller and smaller sets of features. The least important features are pruned from the training and testing set. Cross Validation is used to make a selection of the best number of features.<br>\n",
    "The estimator that will be used is the Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RFECV algorithm\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the RFECV algorithm with the loosely optimized Random Forest classifier\n",
    "f_selector = RFECV(estimator=random_search.best_estimator_ ,\n",
    "                   step=1,\n",
    "                   cv=3,\n",
    "                   n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the RFECV on the training set\n",
    "f_selector = f_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of features vs. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel('Number of features selected')\n",
    "plt.ylabel('Cross-validation accuracy')\n",
    "plt.plot(range(1, len(f_selector.grid_scores_) + 1), f_selector.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best number of features\n",
    "print('Optimal number of features: %i' % (f_selector.n_features_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that shows the ranking of features\n",
    "def create_ranking_df(ranking_list, columns):\n",
    "    ranking = {}\n",
    "    for i in range(len(ranking_list)):\n",
    "        rank = ranking_list[i]\n",
    "        col = columns[i]\n",
    "        if rank in ranking:\n",
    "            ranking[rank].append(col)\n",
    "        else:\n",
    "            ranking[rank] = [col]\n",
    "    ranking = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in ranking.items()]))\n",
    "    return ranking.reindex(sorted(ranking.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show ranking\n",
    "ranking = create_ranking_df(f_selector.ranking_, X.columns)\n",
    "ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Prune redundant featues\n",
    "The selected features will be used to create the new training set and test set.\n",
    "CareerSatisfaction and ConvertedSalary will be added to the selection if they are not selected. This is because the two values are needed to promote or reject the defined hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indeces of best selection of features\n",
    "best_features = [i for i, rank in enumerate(f_selector.ranking_) if rank == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indexes of 'CareerSatisfaction' and 'ConvertedSalary'\n",
    "satisfaction = [i for i, col in enumerate(X.columns) if col == 'CareerSatisfaction'][0]\n",
    "salary= [i for i, col in enumerate(X.columns) if col == 'ConvertedSalary'][0]\n",
    "\n",
    "# If index is not yet in selection: add it to selection\n",
    "if satisfaction not in best_features: best_features.append(satisfaction)\n",
    "if salary not in best_features: best_features.append(salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy_columns = ['CareerSatisfaction',\n",
    "#                 'JobSearchStatus%I am actively looking for a job',\n",
    "#                 'JobSearchStatus%I am not interested in new job...',\n",
    "#                 'JobSearchStatus%Iâ€™m not actively looking, but ...',\n",
    "#                 'ConvertedSalary']\n",
    "#best_features = [i for i, column in  enumerate(X.columns) if column in dummy_columns]\n",
    "X_train_best = X_train[:,best_features]\n",
    "X_test_best = X_test[:,best_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train and optimize different algorithms\n",
    "In this section the following Machine Learning algorithms/methods will be optimized and trained to predict job satisfaction:\n",
    "+ Random Forest (Classifier)\n",
    "+ Support Vector Machine\n",
    "+ Naive Bayes\n",
    "+ Random Forest (Regressor)\n",
    "+ K-means\n",
    "+ Ensemble Learning (combining above algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV algorithm to select best combination of hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import leaning_curve and validation_curve to evaluate trained models\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for plotting learning curves:\n",
    "# Learning curves show the learning rate of the training and \n",
    "# cross-validation set over training examples (m).\n",
    "def plot_learning_curve(estimator, title, X, y, scoring='accuracy'):    \n",
    "    # Fit to get parameters for the learning curve\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator,\n",
    "        X, y, cv=3, train_sizes=np.linspace(.1, 1.0, 5), scoring=scoring, n_jobs=-1)\n",
    "    \n",
    "    # Get mean and standard deviation for training scores\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    \n",
    "    # Get mean and standard deviation for cross-validation scores\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    # Setup chart\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Score (' + scoring + ')')\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color='b')\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color='r')\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color='b',\n",
    "             label='Training score')\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color='r',\n",
    "             label='Cross-validation score')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for plotting validation curves:\n",
    "# Validation curves show the learning rate of the training and \n",
    "# cross-validation set over a range of values of a defined hyperparameter.\n",
    "def plot_validation_curve(estimator, title, X, y, param_range, param_name, scoring='accuracy'):\n",
    "    # Fit to get parameters for the validation curve\n",
    "    train_scores, test_scores = validation_curve(estimator,\n",
    "        X, y, cv=3, param_name=param_name, param_range=param_range, scoring=scoring, n_jobs=-1)\n",
    "    \n",
    "    # Get mean and standard deviation for training scores\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "\n",
    "    # Get mean and standard deviation for cross-validation scores\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    # Setup chart\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('Score (' + scoring + ')')\n",
    "    plt.grid()\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color='g', lw=2)\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                     color='r', lw=2)\n",
    "    plt.plot(param_range, train_scores_mean, 'o-', color='g',\n",
    "         label='Training score')\n",
    "    plt.plot(param_range, test_scores_mean, 'o-', color='r',\n",
    "             label='Cross-validation score')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_metrics(y_test, y_pred, show=True):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', labels=np.unique(y_pred))\n",
    "    precision = precision_score(y_test, y_pred, average='macro', labels=np.unique(y_pred))\n",
    "    recall = recall_score(y_test, y_pred, average='macro', labels=np.unique(y_pred))\n",
    "    if show:\n",
    "        print('Accuracy on test set: %0.3f' % (accuracy))\n",
    "        print('F1-score on test set: %0.3f' % (f1))\n",
    "        print('Precision on test set: %0.3f' % (precision))\n",
    "        print('Recall on test set: %0.3f' % (recall))\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Random Forest (Classifier)\n",
    "*Inherently multiclass*<br><br>\n",
    "The Random Forest classifier is used before when selecting features. This time the Random Forest classifier will be used to predict the job satisfaction using the selected features.\n",
    "First, the default Random Forest classifier will trained on the training set and evaluated.\n",
    "After the evaluation, a second Random Forest will be used to select the optimal hyperparameters.\n",
    "The Random Forest classifier containing the optimal hyperparamaters will be evaluated on high variance (overfitting), high bias (underfitting) and performance based on accuracy.<br>\n",
    "The following hyperparameters will be optimized:\n",
    "+ n_estimators: The number of trees in the forest.\n",
    "+ max_depth: The maximum depth of the tree.\n",
    "+ min_samples_split: The minimum number of samples required to split an internal node.\n",
    "+ min_samples_leaf: The minimum number of samples required to be at a leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.1. Training the default Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train default Random Forest classifier for comparison\n",
    "rf_clf_default = RandomForestClassifier(random_state=21)\n",
    "rf_clf_default_score = np.sum(cross_val_score(rf_clf_default, X_train_best, y_train, cv=5)) / 5\n",
    "\n",
    "# Print default cross-validated accuracy\n",
    "print('Mean cross-validated accuracy of the Random Forest classifier with default hyperparameters: %0.3f' % (rf_clf_default_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show default classification metrics\n",
    "rf_clf_default.fit(X_train_best, y_train)\n",
    "rf_clf_default_metrics = clf_metrics(y_test, rf_clf_default.predict(X_test_best));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plot_learning_curve(RandomForestClassifier(random_state=21),\n",
    "                    'Learning Curves of RF (default)',\n",
    "                    X_train_best, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slight high variance problem (overfitted) will occur when using the default Random Forest classifier. The training set starts of with high accuracy (as expected) but the accuracy doesn't decline much as the number of training examples increases. In contrast, the cross-validation accuracy starts off with low accuracy and increases with very small amounts as the number of training examples increases. This high variance problem can be solved by evaluating different hyperparameters of the Random Forest classifier and using the results to fine-tune the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.2. Evaluate hyperparameters\n",
    "Show the validation curve of different hyperparamethers of the Random Forest classifiers (for both the training and cross-validation set). The results will be used to select a range of hyperparameter to perform Grid Search on, which will optimize the hyperparameters more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show validation curve for different values of hyperparameter 'n_estimators'\n",
    "n_estimators = np.arange(5, 505, 50)\n",
    "plot_validation_curve(RandomForestClassifier(random_state=21), \n",
    "                      'RF accuracy for different values of n_estimator', \n",
    "                      X_train_best, \n",
    "                      y_train, \n",
    "                      n_estimators, \n",
    "                      'n_estimators')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the hyperparameter 'n_estimators' seems to increase the accuracy of the training set between the values 0 and 100, which will add to the high variance problem. Also, increasing 'min_samples_split' doesn't seems to increase the accuracy of the cross-validation set very much. Becuase increasing 'n_estimators' doesn't seem te be necessary, the range between 10 and 200 will be used when performing GridSearch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show validation curve for different values of hyperparameter 'max_depth'\n",
    "max_depth = np.arange(5, 75, 5)\n",
    "plot_validation_curve(RandomForestClassifier(random_state=21), \n",
    "                      'RF accuracy for different values of max_depth', \n",
    "                      X_train_best, \n",
    "                      y_train, \n",
    "                      max_depth, \n",
    "                      'max_depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the hyperparameter 'max_depth' seems to increase the accuracy of the training set between the values 0 and 30, which will add to the high variance problem. Also, increasing 'min_samples_split' does seem to decrease the accuracy of the cross-validation set between the values 0 and 30. Because an increase in 'max_depth' does almost instantly increase the high variance problem and decrease the accuracy, only the value 5 will be used when performing GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show validation curve for different values of hyperparameter 'min_samples_split'\n",
    "min_samples_split = np.arange(5, 755, 30)\n",
    "plot_validation_curve(RandomForestClassifier(random_state=21), \n",
    "                      'RF accuracy for different values of min_samples_split', \n",
    "                      X_train_best, \n",
    "                      y_train, \n",
    "                      min_samples_split, \n",
    "                      'min_samples_split')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the hyperparameter 'min_samples_split' seems to lower the accuracy of the training set, which will reduce the high variance problem. Increasing 'min_samples_split' also seems to increase the accuracy of the cross-validation set, with a plateau between de values 300 and 500. The range between 300 and 500 will therefore be used when performing GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show validation curve for different values of hyperparameter 'min_samples_leaf'\n",
    "min_samples_leaf = np.arange(5, 210, 10)\n",
    "plot_validation_curve(RandomForestClassifier(random_state=21), \n",
    "                      'RF accuracy for different values of min_samples_leaf', \n",
    "                      X_train_best, \n",
    "                      y_train, \n",
    "                      min_samples_leaf, \n",
    "                      'min_samples_leaf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the hyperparameter 'min_samples_leaf' seems to lower the accuracy of the training set, which will reduce the high variance problem. Increasing 'min_samples_leaf' also seems to increase the accuracy of the cross-validation set, with a plateau between de values 60 and 150. The range between 60 and 150 will therefore be used when performing GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.3. Optimize Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the grid\n",
    "rf_grid = { 'n_estimators': np.arange(5, 115, 10),\n",
    "            'max_depth': [5],\n",
    "            'min_samples_split': np.arange(300, 520, 20),\n",
    "            'min_samples_leaf': np.arange(60, 165, 15) }\n",
    "# Print grid\n",
    "[print('%s: %s' % x) for x in rf_grid.items()];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=21)\n",
    "rf_grid_search = GridSearchCV(estimator=rf_clf,\n",
    "                              param_grid=rf_grid,\n",
    "                              cv=5,\n",
    "                              verbose=2,\n",
    "                              n_jobs=-1)\n",
    "# Fit the random search model\n",
    "rf_grid_search = rf_grid_search.fit(X_train_best, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best parameters\n",
    "rf_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best cross-validated accuracy\n",
    "print('Mean cross-validated accuracy of the Random Forest classifier with optimized hyperparameters: %0.3f' % (rf_grid_search.best_score_))\n",
    "\n",
    "# Print improvements relative to Random Forest classfier with default hyperparameters\n",
    "improvement = rf_grid_search.best_score_ - rf_clf_default_score\n",
    "print('Improvement relative to default mean cross-validated accuracy: : %0.3f' % (improvement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Random Forest classifier using the best hyperparameters\n",
    "rf_clf = rf_grid_search.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plot_learning_curve(rf_clf, 'Learning Curves of RF (optimized)', X_train_best, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule of thumb is: The larger the gap between the cross-validation set and the training the set the higher the variance (overfit). After finetuning the hyperparamters of the Random Forest classifier according to the training data, the cross-validation trend and the training set trend converges. This concludes that the problem of overfitting has been solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.4. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf_metrics = clf_metrics(y_test, rf_clf.predict(X_test_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show improvements relative to the default classification metrics\n",
    "print('Accuracy improvement of: %0.3f' % (rf_clf_metrics[0] - rf_clf_default_metrics[0]))\n",
    "print('F1-score improvement of: %0.3f' % (rf_clf_metrics[1] - rf_clf_default_metrics[1]))\n",
    "print('Precision improvement of: %0.3f' % (rf_clf_metrics[2] - rf_clf_default_metrics[2]))\n",
    "print('Recall improvement of: %0.3f' % (rf_clf_metrics[3] - rf_clf_default_metrics[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Support Vector Machine\n",
    "*Multiclass as One-Vs-One*<br><br>\n",
    "The Support Vector Machine classifier will be used to predict the job satisfaction using the selected features.\n",
    "First, the default Support Vector Machine classifier will trained on the training set and evaluated.\n",
    "After the evaluation, a second Support Vector Machine classifier will be used to select the optimal hyperparameters.\n",
    "The Support Vector Machine classifier containing the optimal hyperparamaters will be evaluated on high variance (overfitting), high bias (underfitting) and perfomance based on accuracy.<br>\n",
    "The following hyperparameters will be optimized:\n",
    "+ C: Penalty parameter C of the error term.\n",
    "+ gamma: Kernel coefficient.\n",
    "+ kernel: The kernel type to be used in the algorithm ('linear', 'rbf')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Support Vector Machine module\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.1. Training the default Support Vector Machine classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train default Support Vector Machine classifier for comparison\n",
    "svm_clf_default = SVC(random_state=21)\n",
    "svm_clf_default_score = np.sum(cross_val_score(svm_clf_default, X_train_best, y_train, cv=5, n_jobs=-1)) / 5\n",
    "\n",
    "# Print default cross-validated accuracy\n",
    "print('Mean cross-validated accuracy of the Support Vector Machine classifier with default hyperparameters: %0.3f' % (svm_clf_default_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show default classification metrics\n",
    "svm_clf_default.fit(X_train_best, y_train)\n",
    "svm_clf_default_metrics = clf_metrics(y_test, svm_clf_default.predict(X_test_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial comparison will be made between a Support Vector Machine using the Linear kernel and one using the Gaussian kerntel. The results of this comparison can give better insight into the nature of the data. Is it linearly seperatable or not? If one kernel outperforms the other, the underperforming kernel will be removed from further evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot learning curve using the gaussian (rbf) kernel\n",
    "plot_learning_curve(SVC(kernel='rbf', random_state=21),\n",
    "                    'Learning Curves of SVM with Guassian kernel (default)',\n",
    "                    X_train_best, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default Support Vector Machine classifier with the Gaussian kernel doesn't seem to overfit or underfit the data. The gap between the cross-validations scores and the traing scores are almost non-non-existent. The goal therefore is to only improve the current accuracy by optimizing the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(SVC(kernel='linear', random_state=21),\n",
    "                    'Learning Curves of SVM with Linear kernel (default)', X_train_best, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default Support Vector Machine classifier with the Linear kernel performs way worse than the Support Vector Machine classifier with the Gaussian kernel. This concludes that the data is more non-linearly than it is linearly seperatable. For this reason the Linear kernel won't be used for further evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.2. Evaluate hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the validation curve of different hyperparamethers of the Support Vector Machine classifiers (for both the training and cross-validation set). The results will be used to select a range of hyperparameter to perform Grid Search on, which will optimize the hyperparameters more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show validation curve for different values of hyperparameter 'C'\n",
    "C = np.logspace(0, 3, 4)\n",
    "plot_validation_curve(SVC(kernel='rbf', random_state=21),\n",
    "                      'SVM accuracy for different values of C with Gaussian kernel', \n",
    "                      X_train_best, \n",
    "                      y_train, \n",
    "                      C, \n",
    "                      'C')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the hyperparameter 'C' seems to increase the accuracy of the training set between the values 1000 and 10000. It also looks like the line is still increasing after the value of 10000. The reason that bigger values won't be chosen is because of the processing time it would take and the change of overfitting. The level of C is the level of how much the kernel is trying to fit all the training examples. For these reasons, the values 1000 and 10000 will be chosen when performing GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show validation curve for different values of hyperparameter 'gamma'\n",
    "gamma = np.logspace(0, 3, 4)\n",
    "plot_validation_curve(SVC(kernel='rbf', random_state=21),\n",
    "                      'SVM accuracy for different values of gamma with Gaussian kernel', \n",
    "                      X_train_best, \n",
    "                      y_train, \n",
    "                      gamma, \n",
    "                      'gamma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation set score raises till a gamma value of 100. After this value the cross-validation set score decreases slowly. Also, the training set score keeps raising after the gamme value of 10. This indicates that overfitting can cause an issue if a value is picked above the value of 10 or 100. For these reasons, the values 10 and 100 will be chosen when performing GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.3. Optimize Support Vector Machine classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search will not be necessary for this optimization part. The evaluation part gave a good indication of which values to use as the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the grid\n",
    "svm_grid = { 'C': 1,\n",
    "             'gamma': 10,\n",
    "             'kernel': 'rbf' }\n",
    "# Print grid\n",
    "[print('%s: %s' % x) for x in svm_grid.items()];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit Support Vector Machine with selected hyperparameter values\n",
    "svm_clf = SVC(random_state=21,\n",
    "              C=svm_grid['C'],\n",
    "              gamma=svm_grid['gamma'],\n",
    "              kernel=svm_grid['kernel'])\n",
    "svm_clf = svm_clf.fit(X_train_best, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross-validated accuracy\n",
    "svm_clf_score = np.sum(cross_val_score(svm_clf, X_train_best, y_train, cv=5)) / 5\n",
    "print('Mean cross-validated score of the best estimator: %0.3f' % (svm_clf_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plot_learning_curve(svm_clf, 'Learning Curves of SVM (optimized)', X_train_best, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.4. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf_metrics = clf_metrics(y_test, svm_clf.predict(X_test_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show improvements relative to the default classification metrics\n",
    "print('Accuracy improvement of: %0.3f' % (svm_clf_metrics[0] - svm_clf_default_metrics[0]))\n",
    "print('F1-score improvement of: %0.3f' % (svm_clf_metrics[1] - svm_clf_default_metrics[1]))\n",
    "print('Precision improvement of: %0.3f' % (svm_clf_metrics[2] - svm_clf_default_metrics[2]))\n",
    "print('Recall improvement of: %0.3f' % (svm_clf_metrics[3] - svm_clf_default_metrics[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. Naive Bayes\n",
    "*Inherently multiclass*<br><br>\n",
    "The Naive Bayes classifier will be used to predict the job satisfaction using the selected features.\n",
    "First, the Gaussian Naive Bayes classifier will trained on the training set and evaluated.\n",
    "After the evaluation, the Multinomial Naive Bayes classifier will be trained on the training set and evaluated.\n",
    "Both versions of the Naive Bayes classifier will be compared. The best performing classifier will be chosen. There will be no optimization part because Naive Bayes has no hyperparameters to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Naive Bayes modules\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.1. Training the Gaussian Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gaussian Naive Bayes classifier for comparison\n",
    "gnb_clf = GaussianNB()\n",
    "gnb_clf_score = np.sum(cross_val_score(gnb_clf, X_train_best, y_train, cv=5)) / 5\n",
    "\n",
    "# Print cross-validated accuracy of Gaussian Naive Bayes classifier\n",
    "print('Mean cross-validated accuracy of the Gaussian Naive Bayes classifier: %0.3f' % (gnb_clf_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show classification metrics\n",
    "gnb_clf.fit(X_train_best, y_train)\n",
    "gnb_clf_metrics = clf_metrics(y_test, gnb_clf.predict(X_test_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plot_learning_curve(GaussianNB(), 'Learning Curves of Gaussian Naive Bayes', X_train_best, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.2. Training the Multinomial Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Multinomial Naive Bayes classifier for comparison\n",
    "mnb_clf = MultinomialNB()\n",
    "mnb_clf_score = np.sum(cross_val_score(mnb_clf, X_train_best, y_train, cv=5)) / 5\n",
    "\n",
    "# Print cross-validated accuracy of Multinomial Naive Bayes classifier\n",
    "print('Mean cross-validated score of the Multinomial Naive Bayes: %0.3f' % (mnb_clf_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show classification metrics\n",
    "mnb_clf.fit(X_train_best, y_train)\n",
    "mnb_clf_metrics = clf_metrics(y_test, mnb_clf.predict(X_test_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plot_learning_curve(MultinomialNB(), 'Learning Curves of Multinomial Naive Bayes', X_train_best, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.3. Optimize Naive Bayes classifier\n",
    "No hyperparameters to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf = gnb_clf_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.4. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_clf_metrics = clf_metrics(y_test, gnb_clf.predict(X_test_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4. Random Forest (Regressor)\n",
    "*Regressor*<br><br>\n",
    "The Random Forest regressor will be used to predict the job satisfaction using the selected features.\n",
    "First, the default Random Forest regressor will trained on the training set and evaluated.\n",
    "After the evaluation, a second Random Forest regressor will be used to select the optimal hyperparameters.\n",
    "The Random Forest regressor containing the optimal hyperparamaters will be evaluated on high variance (overfitting), high bias (underfitting) and performance based on the R2 score.<br>\n",
    "The following hyperparameters will be optimized:\n",
    "+ n_estimators: The number of trees in the forest.\n",
    "+ max_depth: The maximum depth of the tree.\n",
    "+ min_samples_split: The minimum number of samples required to split an internal node.\n",
    "+ min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
    "<br><br>\n",
    "Two sets of metrics will be used to measure performance. The first set contains the R2 score, Mean Absolute Error and Mean Squared Error. The second set contains the same metrics as the classifiers (accuracy, f1, recall, precision). The accuracy will be measure by rounding the predicted values of the regressor to represent classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Random Forest Regressor module\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Import metrics\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_metrics(y_test, y_pred, show=True):\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mean_abs_err = mean_absolute_error(y_test, y_pred)\n",
    "    mean_sqr_err = mean_squared_error(y_test, y_pred)\n",
    "    if show:\n",
    "        print('R2 on test set: %0.3f' % (r2))\n",
    "        print('Mean Absolute Error on test set: %0.3f' % (mean_abs_err))\n",
    "        print('Mean Squared Error on test set: %0.3f' % (mean_sqr_err))\n",
    "    return r2, mean_abs_err, mean_sqr_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.1. Training the default Random Forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train default Random Forest regressor for comparison\n",
    "rf_reg_default = RandomForestRegressor(random_state=21)\n",
    "rf_reg_default_score = np.sum(cross_val_score(rf_reg_default, X_train_best, y_train, cv=5, scoring='r2')) / 5\n",
    "\n",
    "# Print default cross-validated R2 score\n",
    "print('Mean cross-validated R2 score of the Random Forest regressor with default hyperparameters: %0.3f' % (rf_reg_default_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show default cross-validated regression metrics\n",
    "rf_reg_default.fit(X_train_best, y_train)\n",
    "rf_reg_default_reg_metrics = reg_metrics(y_test, rf_reg_default.predict(X_test_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show default cross-validated classification metrics\n",
    "y_pred = np.round(rf_reg_default.predict(X_test_best))\n",
    "rf_reg_default_metrics = clf_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plot_learning_curve(RandomForestRegressor(random_state=21), 'Learning Curves of RF regressor (default)',\n",
    "                    X_train_best, y_train,\n",
    "                    scoring='r2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.2. Evaluate hyperparameters\n",
    "Show the validation curve of different hyperparamethers of the Random Forest regressor (for both training and cross-validation set). The results will be used to select a range of hyperparameter to perform GridSearch on, which will fine tune the hyperparameters more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show validation curve for different values of hyperparameter 'n_estimators'\n",
    "n_estimators = np.arange(5, 505, 50)\n",
    "plot_validation_curve(RandomForestRegressor(random_state=21), \n",
    "                      'RF accuracy for different values of n_estimator', \n",
    "                      X_train_best, \n",
    "                      y_train, \n",
    "                      n_estimators, \n",
    "                      'n_estimators',\n",
    "                      scoring='r2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the hyperparameter 'n_estimators' seems to increase the accuracy of the training set between the values 0 and 100, which will add to the high variance problem. Also, increasing 'min_samples_split' doesn't seems to increase the accuracy of the cross-validation set very much. Becuase increasing 'n_estimators' doesn't seem te be necessary, the range between 10 and 200 will be used when performing GridSearch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show validation curve for different values of hyperparameter 'max_depth'\n",
    "max_depth = np.arange(5, 75, 5)\n",
    "plot_validation_curve(RandomForestRegressor(random_state=21), \n",
    "                      'RF accuracy for different values of max_depth', \n",
    "                      X_train_best, \n",
    "                      y_train, \n",
    "                      max_depth, \n",
    "                      'max_depth',\n",
    "                      scoring='r2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the hyperparameter 'max_depth' seems to increase the accuracy of the training set between the values 0 and 30, which will add to the high variance problem. Also, increasing 'min_samples_split' does seem to decrease the accuracy of the cross-validation set between the values 0 and 30. Because an increase in 'max_depth' does almost instantly increase the high variance problem and decrease the accuracy, only the value 5 will be used when performing GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show validation curve for different values of hyperparameter 'min_samples_split'\n",
    "min_samples_split = np.arange(5, 850, 30)\n",
    "plot_validation_curve(RandomForestRegressor(random_state=21), \n",
    "                      'RF accuracy for different values of min_samples_split', \n",
    "                      X_train_best, \n",
    "                      y_train, \n",
    "                      min_samples_split, \n",
    "                      'min_samples_split',\n",
    "                      scoring='r2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the hyperparameter 'min_samples_split' seems to lower the accuracy of the training set, which will reduce the high variance problem. Increasing 'min_samples_split' also seems to increase the accuracy of the cross-validation set, with a plateau between de values 300 and 500. The range between 300 and 500 will therefore be used when performing GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show validation curve for different values of hyperparameter 'min_samples_leaf'\n",
    "min_samples_leaf = np.arange(5, 250, 10)\n",
    "plot_validation_curve(RandomForestRegressor(random_state=21), \n",
    "                      'RF accuracy for different values of min_samples_leaf', \n",
    "                      X_train_best, \n",
    "                      y_train, \n",
    "                      min_samples_leaf, \n",
    "                      'min_samples_leaf',\n",
    "                      scoring='r2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the hyperparameter 'min_samples_leaf' seems to lower the accuracy of the training set, which will reduce the high variance problem. Increasing 'min_samples_leaf' also seems to increase the accuracy of the cross-validation set, with a plateau between de values 60 and 150. The range between 60 and 150 will therefore be used when performing GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3. Optimize Random Forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the grid\n",
    "rf_reg_grid = { 'n_estimators': np.arange(5, 115, 10),\n",
    "                'max_depth': [5],\n",
    "                'min_samples_split': np.arange(400, 850, 50),\n",
    "                'min_samples_leaf': np.arange(100, 275, 25) }\n",
    "# Print grid\n",
    "[print('%s: %s' % x) for x in rf_grid.items()];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor(random_state=21)\n",
    "rf_reg_grid_search = GridSearchCV(estimator=rf_reg,\n",
    "                                  param_grid=rf_reg_grid,\n",
    "                                  cv=5,\n",
    "                                  verbose=2,\n",
    "                                  n_jobs=-1,\n",
    "                                  scoring='r2')\n",
    "# Fit the random search model\n",
    "rf_reg_grid_search = rf_reg_grid_search.fit(X_train_best, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best parameters\n",
    "rf_reg_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best cross-validated score\n",
    "print('R2 score of the best estimator: %0.3f' % (rf_reg_grid_search.best_score_))\n",
    "print('Improvement relative to default R2 score: : %0.3f' % (rf_reg_grid_search.best_score_ - pr_reg_default_score))\n",
    "\n",
    "# Show best parameters\n",
    "best_rf_reg_params = rf_reg_grid_search.best_params_\n",
    "best_rf_reg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Random Forest regressor using the best hyperparameters\n",
    "rf_reg = rf_reg_grid_search.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(rf_reg, 'Learning Curves of RF regressor (optimized)',\n",
    "                    X_train_best, y_train,\n",
    "                    scoring='r2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.4. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg_reg_metrics = reg_metrics(y_test, rf_reg.predict(X_test_best));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show improvements relative to the default cross-validated regression metrics\n",
    "print('R2 score improvement of: %0.3f' % (rf_reg_reg_metrics[0] - rf_reg_default_reg_metrics[0]))\n",
    "print('Mean Absolute Error improvement of: %0.3f' % (rf_reg_reg_metrics[1] - rf_reg_default_reg_metrics[1]))\n",
    "print('Mean Squared Error improvement of: %0.3f' % (rf_reg_reg_metrics[2] - rf_reg_default_reg_metrics[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(rf_reg.predict(X_test_best))\n",
    "rf_reg_metrics = clf_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_reg_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-78338fe812e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Show improvements relative to the default cross-validated classification metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy improvement of: %0.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrf_reg_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrf_reg_default_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'F1-score improvement of: %0.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrf_reg_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrf_reg_default_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Precision improvement of: %0.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrf_reg_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrf_reg_default_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Recall improvement of: %0.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrf_reg_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrf_reg_default_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf_reg_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "# Show improvements relative to the default cross-validated classification metrics\n",
    "print('Accuracy improvement of: %0.3f' % (rf_reg_metrics[0] - rf_reg_default_metrics[0]))\n",
    "print('F1-score improvement of: %0.3f' % (rf_reg_metrics[1] - rf_reg_default_metrics[1]))\n",
    "print('Precision improvement of: %0.3f' % (rf_reg_metrics[2] - rf_reg_default_metrics[2]))\n",
    "print('Recall improvement of: %0.3f' % (rf_reg_metrics[3] - rf_reg_default_metrics[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5. K-means\n",
    "*Clustering*<br><br>\n",
    "K-means will be used to predict the job satisfaction using the selected features.\n",
    "This unsupervised clustering algorithm will search for 7 clusters (target feature has 7 labels). The outputted labels will be permuted. The permutation with the highest performance (accurayc) will be be choses as prediction.<br>\n",
    "K-means has no hyperparameters that could be optimzed. K-means can't also be plotted with a Learning Curve because of it's unsupervised nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import K-means module\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Import metrics\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
    "\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clt_metrics(y_test, y_pred, show=True):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    homogeneity = homogeneity_score(y_test, y_pred)\n",
    "    completeness = completeness_score(y_test, y_pred)\n",
    "    v_measure = v_measure_score(y_test, y_pred)\n",
    "    if show:\n",
    "        print('Accuracy on test set: %0.3f' % (accuracy))\n",
    "        print('Homogeneity score on test set: %0.3f' % (homogeneity))\n",
    "        print('Completeness score on test set: %0.3f' % (completeness))\n",
    "        print('V-Measure score on test set: %0.3f' % (v_measure))\n",
    "    return accuracy, homogeneity, completeness, v_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_most_accurate(y_test, y_pred):\n",
    "    labels = np.zeros_like(y_pred)\n",
    "    for i in range(10):\n",
    "        mask = (y_pred == i)\n",
    "        labels[mask] = mode(y_test[mask])[0]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5.1. Training K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train K-means with 7 clusters (target featuer has 7 labels)\n",
    "km_clt = KMeans(n_clusters=7, random_state=21)\n",
    "km_clt.fit(X_train_best, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5.2. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show clustering metrics\n",
    "y_pred = perm_most_accurate(y_test, km_clt.predict(X_test_best))\n",
    "km_clt_metrics = clt_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6. Ensemble Learning\n",
    "Ensemble Learning will be used to combine the predictive power of all the above Machine Learning algorithms: Random Forest (Classifier), Support Vector Machine with Gaussian kernel, Gaussian Naive Bayes, Random Forest (Regressor) and K-means. All algorithms will do a prediction on the test set and the majority of the predictions will be chosen. If it's an even vote, the prediction will be chosen of the best permorming algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote(y_predictions, accuracies):\n",
    "    y_pred = []\n",
    "    for i in range(len(preds[0])):\n",
    "        y_pred.append([])\n",
    "        for j in range(len(preds)):\n",
    "            y_pred[i].append(preds[j][i])\n",
    "        y_pred[i] = stats.mode(y_pred[i]).mode[0]\n",
    "        # Different votes: look at the best classifier\n",
    "        if y_pred[i] > 1:\n",
    "            y_pred[i] = preds[accs.index(max(accs))][i]\n",
    "    return y_pred          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.6.1. Training Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2d3487c0105e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_best\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvm_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_best\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_best\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_best\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf_clf' is not defined"
     ]
    }
   ],
   "source": [
    "y_predictions = []\n",
    "# Get prediction of Random Forest (Classifier)\n",
    "y_predictions.append(rf_clf.predict(X_test_best))\n",
    "# Get prediction of Support Vector Machine with Gaussian kernel\n",
    "y_predictions.append(svm_clf.predict(X_test_best))\n",
    "# Get prediction of Gaussian Naive Bayes\n",
    "y_predictions.append(nb_clf.predict(X_test_best))\n",
    "# Get prediction of Random Forest (Regressor)\n",
    "y_predictions.append(np.round(rf_reg.predict(X_test_best)))\n",
    "# Get prediction of K-means\n",
    "y_predictions.append(perm_most_accurate(y_test, km_clt.predict(X_test_best)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "# Get accuracy of Random Forest (Classifier)\n",
    "y_predictions.append(clf_metrics(y_test, rf_clf.predict(X_test_best))[0])\n",
    "# Get accuracy of Support Vector Machine with Gaussian kernel\n",
    "y_predictions.append(clf_metrics(y_test, svm_clf.predict(X_test_best))[0])\n",
    "# Get accuracy of Gaussian Naive Bayes\n",
    "y_predictions.append(clf_metrics(y_test, nb_clf.predict(X_test_best))[0])\n",
    "# Get accuracy of Random Forest (Regressor)\n",
    "y_predictions.append(clf_metrics(y_test, np.round(rf_reg.predict(X_test_best)))[0])\n",
    "# Get accuracy of K-means\n",
    "y_predictions.append(clt_metrics(y_test, perm_most_accurate(y_test, km_clt.predict(X_test_best)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = vote(y_predictions, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.6.2. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_metrics = metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare results of Machine Learning algorithms/methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_chart(results, rowIndex, title, y_label):\n",
    "    # Plot all accuracy scores thus far\n",
    "    plt.figure(figsize=(8, 4), dpi=75)\n",
    "\n",
    "    ratios = results.columns\n",
    "    y_pos = np.arange(len(ratios))\n",
    "    performance = results.iloc[rowIndex].values\n",
    "    plt.bar(y_pos, performance, align='center')\n",
    "    plt.xticks(y_pos, ratios)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for name, clf in classifiers:\n",
    "    results[name] = metrics(y_test, clf.predict(X_test_best), show=False)\n",
    "results['Ensemble'] = ensemble_metrics\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_chart(results, 0, 'Accuracy of different classifiers', 'Accuracy in %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_chart(results, 1, 'F1-score of different classifiers', 'F1-score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_chart(results, 2, 'Precision of different classifiers', 'Precision in %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_chart(results, 3, 'Recall of different classifiers', 'Recall in %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
